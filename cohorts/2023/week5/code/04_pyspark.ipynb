{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/23 06:43:02 WARN Utils: Your hostname, yujaeseong-ui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 192.168.219.147 instead (on interface en0)\n",
      "23/02/23 06:43:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/23 06:43:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"../fhv_tripdata_2021-01.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00009', pickup_datetime='2021-01-01 00:27:00', dropoff_datetime='2021-01-01 00:44:00', PULocationID=None, DOLocationID=None, SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00009', pickup_datetime='2021-01-01 00:50:00', dropoff_datetime='2021-01-01 01:07:00', PULocationID=None, DOLocationID=None, SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00013', pickup_datetime='2021-01-01 00:01:00', dropoff_datetime='2021-01-01 01:51:00', PULocationID=None, DOLocationID=None, SR_Flag=None, Affiliated_base_number='B00013'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime='2021-01-01 00:13:09', dropoff_datetime='2021-01-01 00:21:26', PULocationID=None, DOLocationID='72', SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime='2021-01-01 00:38:31', dropoff_datetime='2021-01-01 00:53:44', PULocationID=None, DOLocationID='61', SR_Flag=None, Affiliated_base_number='B00037')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,StringType,true),StructField(DOLocationID,StringType,true),StructField(SR_Flag,StringType,true),StructField(Affiliated_base_number,StringType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.StringType(), True),\n",
    "    types.StructField('dropoff_datetime', types.StringType(), True),\n",
    "    types.StructField('PULocationID', types.LongType(), True),\n",
    "    types.StructField('DOLocationID', types.LongType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(schema) \\\n",
    "        .csv(\"../fhv_tripdata_2021-01.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00009', pickup_datetime='2021-01-01 00:27:00', dropoff_datetime='2021-01-01 00:44:00', PULocationID=None, DOLocationID=None, SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00009', pickup_datetime='2021-01-01 00:50:00', dropoff_datetime='2021-01-01 01:07:00', PULocationID=None, DOLocationID=None, SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00013', pickup_datetime='2021-01-01 00:01:00', dropoff_datetime='2021-01-01 01:51:00', PULocationID=None, DOLocationID=None, SR_Flag=None, Affiliated_base_number='B00013'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime='2021-01-01 00:13:09', dropoff_datetime='2021-01-01 00:21:26', PULocationID=None, DOLocationID=72, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime='2021-01-01 00:38:31', dropoff_datetime='2021-01-01 00:53:44', PULocationID=None, DOLocationID=61, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime='2021-01-01 00:59:02', dropoff_datetime='2021-01-01 01:08:05', PULocationID=None, DOLocationID=71, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime='2021-01-01 00:18:12', dropoff_datetime='2021-01-01 00:30:04', PULocationID=None, DOLocationID=91, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime='2021-01-01 00:36:15', dropoff_datetime='2021-01-01 00:45:08', PULocationID=None, DOLocationID=39, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime='2021-01-01 00:55:04', dropoff_datetime='2021-01-01 01:13:02', PULocationID=None, DOLocationID=37, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime='2021-01-01 00:48:40', dropoff_datetime='2021-01-01 01:12:02', PULocationID=None, DOLocationID=39, SR_Flag=None, Affiliated_base_number='B00037')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.parquet('../fhvhv/2021/01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('../fhvhv/2021/01')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B01312|2021-01-28 16:40:55|2021-01-28 17:28:30|        null|         121|   null|                B03136|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: string, dropoff_datetime: string, PULocationID: bigint, DOLocationID: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter(df.dispatching_base_num == 'B01312')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-28|  2021-01-28|        null|         121|\n",
      "| 2021-01-29|  2021-01-29|        null|         228|\n",
      "| 2021-01-16|  2021-01-16|        null|         225|\n",
      "| 2021-01-06|  2021-01-06|         123|        null|\n",
      "| 2021-01-16|  2021-01-16|        null|         197|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    # base_num = 'B01312'\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s/b44'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_stuff('B02884')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/520| 2021-01-28|  2021-01-28|        null|         121|\n",
      "|  s/afe| 2021-01-29|  2021-01-29|        null|         228|\n",
      "|  s/b21| 2021-01-16|  2021-01-16|        null|         225|\n",
      "|  e/1c1| 2021-01-06|  2021-01-06|         123|        null|\n",
      "|  a/384| 2021-01-16|  2021-01-16|        null|         197|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "conda"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
